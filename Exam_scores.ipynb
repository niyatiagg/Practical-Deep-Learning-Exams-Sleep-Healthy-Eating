{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzwn2se1v0yTtYdnxSTNm0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- FIRST CELL IN COLAB ---\n",
        "import os, random, numpy as np\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
        "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "import tensorflow as tf\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "try:\n",
        "    tf.config.experimental.enable_op_determinism()  # ok if available\n",
        "except Exception:\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "iJZhZ2PQmrDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading,"
      ],
      "metadata": {
        "id": "UDnTGoapF3_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4Los9Mdf_mN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        
        "import pandas as pd, numpy as np, glob, os, random, tensorflow as tf\n",
        "\n",
        "# Set TensorFlow parallelism threads at the very beginning\n",
        "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
        "\n",
        "# 1) Make runs reproducible\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "try:\n",
        "    tf.config.experimental.enable_op_determinism()  # TF 2.12+\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "# ==== 1) Load data ====\n",
        
        "\n",
        "CSV_PATH = \"/content/sample_data/student_exam_scores.csv\"\n",
        "print(\"Using CSV:\", CSV_PATH)\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(df.head(3))\n",
        "\n",
        "#  Set target column \n",
        "TARGET = \"exam_score\"  # <-- change if your column is named differently\n",
        "assert TARGET in df.columns, f\"{TARGET} not found. Columns are: {df.columns.tolist()}\"\n",
        "\n",
        "# ==== 3) Quick EDA (very light) ====\n",
        "print(\"\\nMissing per column (count):\\n\", df.isna().sum())\n",
        "print(\"\\nTarget describe:\\n\", df[TARGET].describe())\n",
        "\n",
        "# ==== 4) Basic cleaning: drop obvious IDs if present (optional) ====\n",
        "for col in [\"student_id\",\"id\",\"Index\",\"index\"]:\n",
        "    if col in df.columns:\n",
        "        df = df.drop(columns=[col])\n",
        "\n",
        "# ==== 5) Split X/y ====\n",
        "y = df[TARGET].astype(float)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "print(\"\\nDataTypes are: \\n\", df.dtypes)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the Train/Val/Test"
      ],
      "metadata": {
        "id": "opyJnDXJG8Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Val/Test split (80/10/10)\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
        "X_train,   X_val,  y_train,   y_val  = train_test_split(X_trainval, y_trainval, test_size=0.1111, random_state=42)\n",
        "# (0.1111 of 90% ≈ 10%, so final ~80/10/10)\n",
        "\n",
        "print(f\"\\nX shapes → train {X_train.shape}, val {X_val.shape}, test {X_test.shape}\")"
      ],
      "metadata": {
        "id": "qj8vfSoXG5PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplest Model (no scaling, no regularisation)"
      ],
      "metadata": {
        "id": "0bhjx3SdEp8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert to float32 arrays for Keras\n",
        "X_train = X_train.astype(\"float32\").values\n",
        "X_val   = X_val.astype(\"float32\").values\n",
        "X_test  = X_test.astype(\"float32\").values\n",
        "y_train = y_train.values.astype(\"float32\")\n",
        "y_val   = y_val.values.astype(\"float32\")\n",
        "y_test  = y_test.values.astype(\"float32\")\n",
        "\n",
        "# ==== 6) Simple deep learning model (Keras MLP) ====\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(32, activation=\"relu\"),\n",
        "    Dense(1)  # regression output\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # MSE loss, MAE metric\n",
        "\n",
        "# Early stopping is basic (not fancy) and prevents overfitting on tiny data\n",
        "es = EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_loss\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,    # with ~200 rows this is fine; you can also set to len(X_train) for full-batch\n",
        "    callbacks=[es]\n",
        ")\n",
        "\n",
        "# ==== 7) Evaluate on test set ====\n",
        "pred = model.predict(X_test, verbose=0).ravel()\n",
        "mae  = mean_absolute_error(y_test, pred)\n",
        "rmse = root_mean_squared_error(y_test, pred)\n",
        "r2   = r2_score(y_test, pred)\n",
        "print(f\"\\nTest MAE:  {mae:.4f}\")\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "print(f\"Test R^2:  {r2:.4f}\")"
      ],
      "metadata": {
        "id": "27FMSTfJcECL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Model with scaled features and regularisation"
      ],
      "metadata": {
        "id": "rM3TWrEOE5i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === (A) Scale features (fit on train, transform val/test) ===\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)  # X_train, X_val, X_test from your split\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "X_train_s = X_train_s.astype(\"float32\")\n",
        "X_val_s   = X_val_s.astype(\"float32\")\n",
        "X_test_s  = X_test_s.astype(\"float32\")\n",
        "\n",
        "# === (B) Simple Keras MLP with mild regularization ===\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train_s.shape[1],)),\n",
        "    Dense(64, activation=\"relu\", kernel_regularizer=l2(1e-4)),\n",
        "    Dropout(0.25),\n",
        "    Dense(32, activation=\"relu\", kernel_regularizer=l2(1e-4)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"mse\",\n",
        "    #tf.keras.losses.Huber(delta=1.0),   # robust regression loss\n",
        "    metrics=[\"mae\"]\n",
        ")\n",
        "\n",
        "#es  = EarlyStopping(monitor=\"val_mae\", patience=10, restore_best_weights=True)\n",
        "es = EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=8, restore_best_weights=True, min_delta=1e-4\n",
        ")\n",
        "\n",
        "rlr = ReduceLROnPlateau(monitor=\"val_mae\", factor=0.5, patience=1, min_lr=1e-5)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_s, y_train,\n",
        "    validation_data=(X_val_s, y_val),\n",
        "    epochs=400,\n",
        "    batch_size=min(32, len(X_train_s)),  # tiny dataset → small batch\n",
        "    shuffle=False,\n",
        "    verbose=0,\n",
        "    callbacks=[es, rlr]\n",
        ")\n",
        "\n",
        "for i, (tr, vl) in enumerate(zip(history.history[\"loss\"], history.history[\"val_loss\"]), start=1):\n",
        "    print(f\"Epoch {i:03d} | train_loss {tr:.4f} | val_loss {vl:.4f}\")\n",
        "\n",
        "# === (C) Test metrics ===\n",
        "pred = model.predict(X_test_s).ravel()\n",
        "mae  = mean_absolute_error(y_test, pred)\n",
        "rmse = root_mean_squared_error(y_test, pred)\n",
        "r2   = r2_score(y_test, pred)\n",
        "print(f\"Test MAE:  {mae:.4f}\")\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "print(f\"Test R^2:  {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "biLf8pzlb8cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ---- 1) Permutation importance for a Keras regressor ----\n",
        "def permutation_importance_keras(model, X_s, y, n_repeats=20, seed=42):\n",
        "    \"\"\"\n",
        "    X_s: 2D numpy array (scaled features) of shape (n_samples, n_features)\n",
        "    y:   1D numpy array of true targets\n",
        "    returns: (importances, baseline_mae)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    base_pred = model.predict(X_s, verbose=0).ravel()\n",
        "    baseline_mae = mean_absolute_error(y, base_pred)\n",
        "\n",
        "    Xw = X_s.copy()\n",
        "    imps = np.zeros(X_s.shape[1], dtype=float)\n",
        "\n",
        "    for j in range(X_s.shape[1]):\n",
        "        scores = []\n",
        "        saved_col = Xw[:, j].copy()\n",
        "        for _ in range(n_repeats):\n",
        "            idx = rng.permutation(len(y))\n",
        "            Xw[:, j] = saved_col[idx]          # shuffle column j\n",
        "            pred = model.predict(Xw, verbose=0).ravel()\n",
        "            scores.append(mean_absolute_error(y, pred))\n",
        "        imps[j] = np.mean(scores) - baseline_mae\n",
        "        Xw[:, j] = saved_col                  # restore column\n",
        "    return imps, baseline_mae\n",
        "\n",
        "# feature names (after get_dummies)\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "# compute importance on TEST set (you can use VAL instead)\n",
        "imps, base_mae = permutation_importance_keras(model, X_test_s, y_test, n_repeats=30, seed=42)\n",
        "\n",
        "imp_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"mae_increase\": imps\n",
        "}).sort_values(\"mae_increase\", ascending=False)\n",
        "\n",
        "print(f\"Baseline MAE on test: {base_mae:.4f}\")\n",
        "display(imp_df.head(5))\n"
      ],
      "metadata": {
        "id": "Tsy9SYlkUW0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-fold cross validation for a simple Keras regressor"
      ],
      "metadata": {
        "id": "7cnm27b3E_3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== N-fold cross validation for a simple Keras regressor =====\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# --- reproducibility (simple) ---\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "# X, y should already exist (X = pd.get_dummies(...); y = df[TARGET].astype(float))\n",
        "X_arr = X.astype(\"float32\").values\n",
        "y_arr = y.values.astype(\"float32\")\n",
        "\n",
        "K = 5  # <-- change to your desired number of folds (e.g., 5 or 10)\n",
        "\n",
        "def build_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(64, activation=\"relu\", kernel_regularizer=l2(1e-4)),\n",
        "        Dropout(0.25),\n",
        "        Dense(32, activation=\"relu\", kernel_regularizer=l2(1e-4)),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss=tf.keras.losses.Huber(delta=3.0),  # or \"mse\"\n",
        "                  metrics=[\"mae\"])\n",
        "    return model\n",
        "\n",
        "kf = KFold(n_splits=K, shuffle=True, random_state=SEED)\n",
        "\n",
        "maes, rmses, r2s = [], [], []\n",
        "\n",
        "fold = 0\n",
        "for train_idx, val_idx in kf.split(X_arr, y_arr):\n",
        "    fold += 1\n",
        "    X_tr, X_va = X_arr[train_idx], X_arr[val_idx]\n",
        "    y_tr, y_va = y_arr[train_idx], y_arr[val_idx]\n",
        "\n",
        "    # Scale features *inside each fold* (fit on train, transform val)\n",
        "    sc = StandardScaler()\n",
        "    X_trs = sc.fit_transform(X_tr).astype(\"float32\")\n",
        "    X_vas = sc.transform(X_va).astype(\"float32\")\n",
        "\n",
        "    model = build_model(X_trs.shape[1])\n",
        "\n",
        "    es  = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, min_delta=1e-4)\n",
        "    rlr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_trs, y_tr,\n",
        "        validation_data=(X_vas, y_va),\n",
        "        epochs=400,\n",
        "        batch_size=min(32, len(X_trs)),\n",
        "        shuffle=False,\n",
        "        verbose=0,\n",
        "        callbacks=[es, rlr]\n",
        "    )\n",
        "\n",
        "    # Evaluate on the held-out fold\n",
        "    pred = model.predict(X_vas, verbose=0).ravel()\n",
        "    mae  = mean_absolute_error(y_va, pred)\n",
        "    rmse = root_mean_squared_error(y_va, pred)\n",
        "    r2   = r2_score(y_va, pred)\n",
        "\n",
        "    maes.append(mae); rmses.append(rmse); r2s.append(r2)\n",
        "    print(f\"Fold {fold}/{K} -> MAE {mae:.4f} | RMSE {rmse:.4f} | R^2 {r2:.4f} | epochs {len(history.history['loss'])}\")\n",
        "\n",
        "print(\"\\n==== Cross-validated results ====\")\n",
        "print(f\"MAE : {np.mean(maes):.4f} ± {np.std(maes):.4f}\")\n",
        "print(f\"RMSE: {np.mean(rmses):.4f} ± {np.std(rmses):.4f}\")\n",
        "print(f\"R^2 : {np.mean(r2s):.4f} ± {np.std(r2s):.4f}\")\n"
      ],
      "metadata": {
        "id": "FGvLpYZ5FCR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# ---- 1) Permutation importance for a Keras regressor ----\n",
        "def permutation_importance_keras(model, X_s, y, n_repeats=20, seed=42):\n",
        "    \"\"\"\n",
        "    X_s: 2D numpy array (scaled features) of shape (n_samples, n_features)\n",
        "    y:   1D numpy array of true targets\n",
        "    returns: (importances, baseline_mae)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    base_pred = model.predict(X_s, verbose=0).ravel()\n",
        "    baseline_mae = mean_absolute_error(y, base_pred)\n",
        "\n",
        "    Xw = X_s.copy()\n",
        "    imps = np.zeros(X_s.shape[1], dtype=float)\n",
        "\n",
        "    for j in range(X_s.shape[1]):\n",
        "        scores = []\n",
        "        saved_col = Xw[:, j].copy()\n",
        "        for _ in range(n_repeats):\n",
        "            idx = rng.permutation(len(y))\n",
        "            Xw[:, j] = saved_col[idx]          # shuffle column j\n",
        "            pred = model.predict(Xw, verbose=0).ravel()\n",
        "            scores.append(mean_absolute_error(y, pred))\n",
        "        imps[j] = np.mean(scores) - baseline_mae\n",
        "        Xw[:, j] = saved_col                  # restore column\n",
        "    return imps, baseline_mae\n",
        "\n",
        "# feature names (after get_dummies)\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "# compute importance on TEST set (you can use VAL instead)\n",
        "imps, base_mae = permutation_importance_keras(model, X_vas, y_va, n_repeats=30, seed=42)\n",
        "\n",
        "imp_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"mae_increase\": imps\n",
        "}).sort_values(\"mae_increase\", ascending=False)\n",
        "\n",
        "print(f\"Baseline MAE on test: {base_mae:.4f}\")\n",
        "display(imp_df.head(5))\n"
      ],
      "metadata": {
        "id": "DbmwI6tv5fJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_va.std(), y_va.min(), y_va.max()\n",
        "nrmse = 3.0614 / y_va.std()  # RMSE / std of y_va\n",
        "perc_range = 2.5437 / (y_va.max() - y_va.min())\n",
        "print(\"NRMSE:\", nrmse, \"MAE as % of range:\", perc_range)"
      ],
      "metadata": {
        "id": "VBsgWfupH1lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.std(), y_test.min(), y_test.max()\n",
        "nrmse = 3.0614 / y_test.std()  # RMSE / std of y_test\n",
        "perc_range = 2.5437 / (y_test.max() - y_test.min())\n",
        "print(\"NRMSE:\", nrmse, \"MAE as % of range:\", perc_range)\n"
      ],
      "metadata": {
        "id": "2yid-ibZ5vB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "h = history.history\n",
        "plt.plot(h[\"loss\"], label=\"train\")\n",
        "plt.plot(h[\"val_loss\"], label=\"val\")\n",
        "plt.title(\"Learning curves (loss)\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "7OBK65Emdbrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "train_pred = model.predict(X_train_s).ravel()\n",
        "val_pred   = model.predict(X_val_s).ravel()\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_pred)\n",
        "val_mae   = mean_absolute_error(y_val,   val_pred)\n",
        "\n",
        "print(f\"Train MAE: {train_mae:.3f}   Val MAE: {val_mae:.3f}\")\n"
      ],
      "metadata": {
        "id": "Fto9BjivdkYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
